{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "spark_session = SparkSession.builder.appName(\"demo\").config(conf=SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_from_json_file(spark):\n",
    "    df = spark.read.json(\"./resources/people.json\")\n",
    "    #显示表内容\n",
    "    df.show()\n",
    "    #显示表信息\n",
    "    df.printSchema()\n",
    "    #显示某列的信息\n",
    "    df.select(\"name\").show()\n",
    "    #显示名字和年龄，年龄加1\n",
    "    df.select(df['name'], df['age'] + 1).show()\n",
    "    #只显示年龄大于17的人\n",
    "    df.filter(df['age'] > 17).show()\n",
    "    #按年龄显示人数\n",
    "    df.groupBy(df['age']).count().show()\n",
    "    #使用sql查询语句\n",
    "    df.createOrReplaceTempView(\"people\")\n",
    "    \n",
    "    sqlDF = spark.sql(\"select * from people where age > 17\")\n",
    "    #显示结果\n",
    "    sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n",
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_df_from_json_file(spark_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从上面结果看到，Michael的年龄值为null,SparkSQL里的DataFrame也有fillna()的方法。我们可以调用df.fillna(13),或者df.na.fill(13)可以将null值填充为13, 如果一个DataFrame中有多个列的值为null, 也可用df.fill({'列名1':值1， '列名2':值2})来一起填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 13|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fill na:\n",
    "df_test = spark_session.read.json(\"./resources/people.json\")\n",
    "df_test = df_test.na.fill(13)\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "def rdd_to_df_way_one(spark):\n",
    "    sc = spark.sparkContext\n",
    "    line_rdd = sc.textFile(\"./resources/people.txt\")\n",
    "    peoples = line_rdd.map(lambda x: x.split(\",\")).map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "    \n",
    "    people_df = spark.createDataFrame(peoples)\n",
    "    \n",
    "    people_df.createOrReplaceTempView(\"people_df\")\n",
    "    \n",
    "    data = spark.sql(\"select * from people_df\")\n",
    "    data.show()\n",
    "    #data是DataFrame，将其转成rdd\n",
    "    data_name = data.rdd.map(lambda x:x.name).collect()\n",
    "    for name in data_name:\n",
    "        print(\"name :\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "+---+-------+\n",
      "\n",
      "name : Michael\n",
      "name : Andy\n",
      "name : Justin\n"
     ]
    }
   ],
   "source": [
    "rdd_to_df_way_one(spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "def rdd_to_df_way_second(spark):\n",
    "    sc = spark.sparkContext\n",
    "    #读文件，生成rdd\n",
    "    line = sc.textFile(\"./resources/people.txt\")\n",
    "    parts = line.map(lambda x : x.split(\",\")).map(lambda x : (x[0], x[1].strip()))\n",
    "    \n",
    "    schema_str = \"name age\"\n",
    "    fields = [StructField(field_name, StringType(), True) for field_name in schema_str.split()]\n",
    "    schema = StructType(fields)\n",
    "    \n",
    "    people_df = spark.createDataFrame(parts, schema)\n",
    "    \n",
    "    people_df.createOrReplaceTempView(\"people_1\")\n",
    "    \n",
    "    sql_result = spark.sql(\"select * from people_1 where age = 30\")\n",
    "    sql_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|Andy| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_to_df_way_second(spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
